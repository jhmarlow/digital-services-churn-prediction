{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting User Churn with Apache Spark and AWS EMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains data cleaning, feature creation and ML optimization for the Sparkify dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20200828100649-0003\n",
      "KERNEL_ID = 5c26be5a-e536-469c-896d-681990c5b6c2\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# pyspark sql\n",
    "from pyspark.sql import SparkSession, Window, functions as F\n",
    "from pyspark.sql.functions import from_unixtime, udf, col, when, isnan, desc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import min, max\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# pyspark ml\n",
    "from pyspark.ml.feature import VectorAssembler, Normalizer, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "#from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python imports\n",
    "import datetime\n",
    "from typing import NewType\n",
    "pysparkdf = NewType('pysparkdf', object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ibm config\n",
    "import ibmos2spark\n",
    "\n",
    "# config\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-147e1161-7da9-41fe-ac00-c144730def00',\n",
    "    'iam_service_endpoint': 'https://iam.cloud.ibm.com/oidc/token',\n",
    "    'api_key': 'kAtvjdC8VIYYUmU3gDaOYIK2fCvP3nkjYYlDiNuu4gw6'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_76774389dfa04fb5acbb1640b3e11704_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Spark version 2.3.4\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"User Churn\").getOrCreate()\n",
    "\n",
    "# Read in data from IBM Cloud\n",
    "data_df = spark.read.json(cos.url('medium-sparkify-event-data.json', 'sparkify-donotdelete-pr-fnqu5byx41gcai'))\n",
    "\n",
    "print(\"Started Spark version {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Martin Orford</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Joseph</td>\n",
       "      <td>M</td>\n",
       "      <td>20</td>\n",
       "      <td>Morales</td>\n",
       "      <td>597.55057</td>\n",
       "      <td>free</td>\n",
       "      <td>Corpus Christi, TX</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1532063507000</td>\n",
       "      <td>292</td>\n",
       "      <td>Grand Designs</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352011000</td>\n",
       "      <td>\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Brown's Body</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Sawyer</td>\n",
       "      <td>M</td>\n",
       "      <td>74</td>\n",
       "      <td>Larson</td>\n",
       "      <td>380.21179</td>\n",
       "      <td>free</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1538069638000</td>\n",
       "      <td>97</td>\n",
       "      <td>Bulls</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352025000</td>\n",
       "      <td>\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              artist       auth firstName gender  itemInSession lastName  \\\n",
       "0      Martin Orford  Logged In    Joseph      M             20  Morales   \n",
       "1  John Brown's Body  Logged In    Sawyer      M             74   Larson   \n",
       "\n",
       "      length level                              location method      page  \\\n",
       "0  597.55057  free                    Corpus Christi, TX    PUT  NextSong   \n",
       "1  380.21179  free  Houston-The Woodlands-Sugar Land, TX    PUT  NextSong   \n",
       "\n",
       "    registration  sessionId           song  status             ts  \\\n",
       "0  1532063507000        292  Grand Designs     200  1538352011000   \n",
       "1  1538069638000         97          Bulls     200  1538352025000   \n",
       "\n",
       "                                           userAgent userId  \n",
       "0  \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...    293  \n",
       "1  \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...     98  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Row 1:\")\n",
    "data_df = df\n",
    "data_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(count(DISTINCT userId)=449)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count users\n",
    "print(\"Number of users:\")\n",
    "data_df.agg(countDistinct(\"userId\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of log events:\n",
      "(543705, 18)\n"
     ]
    }
   ],
   "source": [
    "# count events\n",
    "print(\"Number of log events:\")\n",
    "print((data_df.count(), len(data_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start and end unix time:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(min(ts)=1538352011000, max(ts)=1543622466000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"start and end unix time:\")\n",
    "data_df.select(min(\"ts\"), max(\"ts\")).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(data_df):\n",
    "    \"\"\"Remove non useful columns and drop missing values for user and session.\n",
    "    \"\"\"\n",
    "    # lets remove some of the columns we don't think will be useful from data exploration\n",
    "    cols_to_drop = ['firstName', 'lastName','artist', 'song', 'method', 'status', 'userAgent']\n",
    "    user_log_df = data_df.drop(*cols_to_drop)\n",
    "    \n",
    "    # drop rows with missing info\n",
    "    return user_log_df.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])\n",
    "\n",
    "def unix_to_datetime(data_df):\n",
    "    \"\"\" onvert unix timestamps to datetime.\n",
    "    \"\"\"\n",
    "    # event unix to datetime\n",
    "    data_df = data_df.withColumn(\"timestampDatetime\",\n",
    "                                         from_unixtime(user_log_valid.ts/1000,\n",
    "                                                       format='yyyy-MM-dd HH:mm:ss'))\n",
    "    # registration unix to datetime\n",
    "    data_df = data_df.withColumn(\"registrationDatetime\",\n",
    "                                         from_unixtime(user_log_valid.registration/1000,\n",
    "                                                       format='yyyy-MM-dd HH:mm:ss'))\n",
    "    return data_df\n",
    "\n",
    "def create_us_states(data_df):\n",
    "    \"\"\"Create US states column from location.\n",
    "    \"\"\"\n",
    "    # we don't really want to drop these rows as the col isn't vital \n",
    "    # so replace missing values to allow split\n",
    "    data_df = data_df.fillna({'location':'unknown'})\n",
    "\n",
    "    # define UDFs\n",
    "    # create state column\n",
    "    loc_split = udf(lambda x: x.split(', ')[-1], StringType())\n",
    "\n",
    "    # Sates seem to be appended, so take latest\n",
    "    state_split = udf(lambda x: x.split('-')[-1], StringType())\n",
    "\n",
    "    # apply UDFs\n",
    "    data_df = data_df.withColumn(\"usStateAbbr\",\n",
    "                                 when(data_df.location.isNotNull(),\n",
    "                                      loc_split(data_df.location)).otherwise(''))\n",
    "    data_df = data_df.withColumn(\"usStateAbbr\",\n",
    "                                 when(data_df.usStateAbbr.isNotNull(),\n",
    "                                      state_split(data_df.usStateAbbr)).otherwise(''))\n",
    "    return data_df\n",
    "\n",
    "def replace_missing_gender(data_df):\n",
    "    \"\"\"Replace missing gender with 'unknown'.\n",
    "    \"\"\"\n",
    "    return data_df.fillna({'gender':'unknown'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_valid = clean_df(data_df)\n",
    "user_log_valid = unix_to_datetime(user_log_valid)\n",
    "user_log_valid = create_us_states(user_log_valid)\n",
    "user_log_valid = replace_missing_gender(user_log_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_data = user_log_valid.withColumnRenamed(\"auth\", \"authLevel\").\\\n",
    "                               withColumnRenamed(\"length\", \"sessionLength_s\").\\\n",
    "                               withColumnRenamed(\"level\", \"subLevel\").\\\n",
    "                               withColumnRenamed(\"ts\", \"unixEventTS\").\\\n",
    "                               withColumnRenamed(\"registration\", \"unixRegistrationTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_phase_feature(data_df: pysparkdf) -> pysparkdf:\n",
    "    \"\"\"Use the cancellation to identify churned users.\n",
    "    \"\"\"\n",
    "    # flag any cancellation confirmation events in pages\n",
    "    flag_cancellation_event = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "    data_df = data_df.withColumn(\"churn\", flag_cancellation_event(\"page\"))\n",
    "\n",
    "    # search for flags and fill user rows as churned\n",
    "    windowval = Window.partitionBy(\"userId\").orderBy(desc(\"unixEventTS\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "    return data_df.withColumn(\"label\", Fsum(\"churn\").over(windowval))\n",
    "\n",
    "def avg_items_in_session(data_df: pysparkdf)-> pysparkdf:\n",
    "    \"\"\"Calculate avg items in session for each user.\n",
    "    \"\"\"\n",
    "    # calculate metric and join dfs back together\n",
    "    return data_df.join(data_df.groupBy('userId').avg('itemInSession'),\n",
    "                               on='userId')\n",
    "\n",
    "def avg_user_listening_time(data_df: pysparkdf) -> pysparkdf:\n",
    "    \"\"\"Calculate average listening time.\n",
    "    \"\"\"\n",
    "    return data_df.join(data_df.groupBy('userId').avg('sessionLength_s'),\n",
    "                               on='userId')\n",
    "\n",
    "def recommendation_performance_good(data_df: pysparkdf) -> pysparkdf:\n",
    "    \"\"\"Number of positive recommendation events.\n",
    "    \"\"\"\n",
    "    # flag events\n",
    "    data_df = data_df.withColumn(\"recc_performance_good_events\",\n",
    "                                 when((data_df[\"page\"] == 'Add to Playlist') |\\\n",
    "                                      (data_df[\"page\"] == 'Add Friend') |\\\n",
    "                                      (data_df[\"page\"] == 'Thumbs Up'),\n",
    "                                       1).otherwise(0))\n",
    "    # calculate number\n",
    "    return data_df.join(data_df.groupBy('userId').sum('recc_performance_good_events'),\n",
    "                               on='userId')\n",
    "\n",
    "def recommendation_performance_bad(data_df: pysparkdf) -> pysparkdf:\n",
    "    \"\"\"Number of bad recommendation events.\n",
    "    \"\"\"\n",
    "    # flag events\n",
    "    data_df = data_df.withColumn(\"recc_performance_bad_events\",\n",
    "                                 when((data_df[\"page\"] == 'Thumbs Down'),\n",
    "                                       1).otherwise(0))\n",
    "    # calculate number\n",
    "    return data_df.join(data_df.groupBy('userId').sum('recc_performance_bad_events'),\n",
    "                               on='userId')\n",
    "\n",
    "def system_performance_bad(data_df: pysparkdf) -> pysparkdf:\n",
    "    \"\"\"Number of bad system events.\n",
    "    \"\"\"\n",
    "    # flag events\n",
    "    data_df = data_df.withColumn(\"sys_performance_bad\",\n",
    "                                 when((data_df[\"page\"] == 'Help') |\\\n",
    "                                      (data_df[\"page\"] == 'Upgrade') |\\\n",
    "                                      (data_df[\"page\"] == 'Error'),\n",
    "                                       1).otherwise(0))\n",
    "    # calculate number                                \n",
    "    return data_df.join(data_df.groupBy('userId').sum('sys_performance_bad'),\n",
    "                               on='userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features\n",
    "wrangled_data = create_phase_feature(wrangled_data)\n",
    "wrangled_data = avg_items_in_session(wrangled_data)\n",
    "wrangled_data = avg_user_listening_time(wrangled_data)\n",
    "wrangled_data = recommendation_performance_good(wrangled_data)\n",
    "wrangled_data = recommendation_performance_bad(wrangled_data)\n",
    "wrangled_data = system_performance_bad(wrangled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ml features df\n",
    "ml_features_data = wrangled_data.select('label',\n",
    "                                        'userId',\n",
    "                                        'gender',\n",
    "                                        'usStateAbbr',\n",
    "                                        col('avg(itemInSession)').alias(\"avg_item_in_session\"),\n",
    "                                        col('avg(sessionLength_s)').alias('avg_session_length'),\n",
    "                                        col('sum(recc_performance_good_events)').alias('num_good_recc'),\n",
    "                                        col('sum(recc_performance_bad_events)').alias('num_bad_recc'),\n",
    "                                        col('sum(sys_performance_bad)').alias('num_bad_sys'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_features_data = ml_features_data.drop_duplicates()  # remove duplicates as features will be created on each event row\n",
    "data_df.unpersist();  # clean up to free up space\n",
    "wrangled_data.unpersist();  # clean up to free up space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>userId</th>\n",
       "      <th>gender</th>\n",
       "      <th>usStateAbbr</th>\n",
       "      <th>avg_item_in_session</th>\n",
       "      <th>avg_session_length</th>\n",
       "      <th>num_good_recc</th>\n",
       "      <th>num_bad_recc</th>\n",
       "      <th>num_bad_sys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100010</td>\n",
       "      <td>F</td>\n",
       "      <td>CT</td>\n",
       "      <td>33.839416</td>\n",
       "      <td>269.489070</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>200002</td>\n",
       "      <td>M</td>\n",
       "      <td>WI</td>\n",
       "      <td>54.832911</td>\n",
       "      <td>253.857506</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  userId gender usStateAbbr  avg_item_in_session  avg_session_length  \\\n",
       "0      1  100010      F          CT            33.839416          269.489070   \n",
       "1      1  200002      M          WI            54.832911          253.857506   \n",
       "\n",
       "   num_good_recc  num_bad_recc  num_bad_sys  \n",
       "0              8             3            2  \n",
       "1             23             5            3  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_features_data.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoder(ml_features_data):\n",
    "    ## https://stackoverflow.com/questions/32277576/how-to-handle-categorical-features-with-spark-ml\n",
    "\n",
    "    cols = ['gender', 'usStateAbbr']\n",
    "\n",
    "    indexers = [\n",
    "        StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "        for c in cols\n",
    "    ]\n",
    "\n",
    "    encoders = [\n",
    "        OneHotEncoder(\n",
    "            inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "        for indexer in indexers\n",
    "    ]\n",
    "\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[encoder.getOutputCol() for encoder in encoders],\n",
    "        outputCol=\"features2\"\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "    return pipeline.fit(ml_features_data).transform(ml_features_data)\n",
    "\n",
    "ml_features_data = onehot_encoder(ml_features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of ML table:\n",
      "(449, 14)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of ML table:')\n",
    "print((ml_features_data.count(), len(ml_features_data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_features_data = ml_features_data.na.drop()  # catch any remaining nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of ML table:\n",
      "(448, 14)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of ML table:')\n",
    "print((ml_features_data.count(), len(ml_features_data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_assembler(ml_features_data):\n",
    "\n",
    "    # this vector is created in prep for ml\n",
    "    assembler = VectorAssembler(inputCols=[\"avg_item_in_session\",\n",
    "                                           \"avg_session_length\",\n",
    "                                           \"num_good_recc\",\n",
    "                                           \"num_bad_recc\",\n",
    "                                           \"num_bad_sys\",\n",
    "                                           'gender_indexed',\n",
    "                                           'usStateAbbr_indexed'],\n",
    "                                outputCol=\"raw_features\",)\n",
    "                                #handleInvalid=\"skip\")\n",
    "\n",
    "    return assembler.transform(ml_features_data)\n",
    "\n",
    "ml_features_data = vector_assembler(ml_features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaler(ml_features_data):\n",
    "    \"\"\"apply scaler\n",
    "    \"\"\"\n",
    "    scaler = Normalizer(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "    return scaler.transform(ml_features_data)\n",
    "\n",
    "ml_df = feature_scaler(ml_features_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_set_evaluator(train):\n",
    "    \"\"\"Check for data skew\n",
    "    \"\"\"\n",
    "    churned_count = train.filter(train['label']==1).count()\n",
    "    non_churned_count = train.filter(train['label']==0).count()\n",
    "    \n",
    "    print(\"{} churned users\".format(churned_count))\n",
    "    print(\"{} non-churned users\".format(non_churned_count))\n",
    "    print(\"{} ratio of churned/non-churned users\".format(churned_count/non_churned_count))\n",
    "\n",
    "def custom_evaluation(results):\n",
    "    \"\"\"Customer function to evaluate result.\n",
    "    \"\"\"\n",
    "    # Generic evaluation\n",
    "    total_results = results.count()\n",
    "    correct_pred = results.filter(results.label == results.prediction).count()\n",
    "    incorrect_pred = results.filter(results.label != results.prediction).count()\n",
    "    \n",
    "    print('Total users predicted correctly: {}'.format(correct_pred))\n",
    "    print('Total users predicted wrongly: {}'.format(incorrect_pred))\n",
    "    print(\"Percentage predicted correct (%): {} \\n\".format((correct_pred/total_results)*100))\n",
    "    \n",
    "    # Correct churn predictions\n",
    "    churn_correct = results.filter((results.label == 1) & (results.prediction == 1)).count()\n",
    "    actual_churned_users = results.filter(results.label == 1).count()\n",
    "    print('User churned and predicted to churn: {}'.format(churn_correct))\n",
    "    print('Total users churned : {}'.format(actual_churned_users))\n",
    "    print('Percent churned user events predicted correctly(%): {}\\n'.format((churn_correct/actual_churned_users)*100))\n",
    "    \n",
    "    # Incorrect churn predictions\n",
    "    churn_incorrect = results.filter((results.label == 0) & (results.prediction == 1)).count()\n",
    "    print('User did not churn and predicted to: {}'.format(churn_incorrect))\n",
    "    print('Percent churned users predicted incorrectly(%): {}\\n'.format((churn_incorrect/total_results)*100))\n",
    "\n",
    "def model_metrics(results):\n",
    "    # ref: https://stackoverflow.com/questions/60772315/how-to-evaluate-a-classifier-with-apache-spark-2-4-5-and-pyspark-python\n",
    "\n",
    "    # Create both evaluators\n",
    "    evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"target\",\n",
    "                                                       predictionCol=\"prediction\")\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"target\",\n",
    "                                              rawPredictionCol=\"prediction\",\n",
    "                                              metricName='areaUnderROC')\n",
    "    \n",
    "    # Make predicitons\n",
    "    results_cast = results.withColumn(\"target\", col(\"label\").cast(DoubleType()))\n",
    "    predictionAndTarget = results_cast.select(\"target\",\"prediction\")\n",
    "\n",
    "    # Get metrics\n",
    "    acc = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "    print(\"Accuracy: {}\".format(acc))\n",
    "    \n",
    "    f1 = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"f1\"})\n",
    "    print(\"F1Score: {}\".format(f1))\n",
    "     \n",
    "    weightedPrecision = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedPrecision\"})\n",
    "    print(\"WeightedPrecision: {}\".format(weightedPrecision))\n",
    "\n",
    "    weightedRecall = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedRecall\"})\n",
    "    print(\"WeightedRecall: {}\".format(weightedRecall))\n",
    "    \n",
    "    auc = evaluator.evaluate(predictionAndTarget)\n",
    "    print(\"AUC: {}\".format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df = ml_df.select(['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.1243353042150227, 0.9901768242441354, 0.029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.2102401746006758, 0.9733396424936257, 0.088...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           features\n",
       "0      1  [0.1243353042150227, 0.9901768242441354, 0.029...\n",
       "1      1  [0.2102401746006758, 0.9733396424936257, 0.088..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split for ML validation\n",
    "train, test =  ml_df.randomSplit([0.7, 0.3], seed=42)  # more equal fit to combat overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mini = train.limit(50)\n",
    "test_mini = test.limit(50)\n",
    "train = train_mini\n",
    "test = test_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 churned users\n",
      "39 non-churned users\n",
      "0.28205128205128205 ratio of churned/non-churned users\n"
     ]
    }
   ],
   "source": [
    "train_set_evaluator(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logistic Regression Model\n"
     ]
    }
   ],
   "source": [
    "print('Baseline Logistic Regression Model')\n",
    "lr_model = LogisticRegression().fit(train)\n",
    "lr_results = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users predicted correctly: 35\n",
      "Total users predicted wrongly: 15\n",
      "Percentage predicted correct (%): 70.0 \n",
      "\n",
      "User churned and predicted to churn: 5\n",
      "Total users churned : 14\n",
      "Percent churned user events predicted correctly(%): 35.714285714285715\n",
      "\n",
      "User did not churn and predicted to: 6\n",
      "Percent churned users predicted incorrectly(%): 12.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_evaluation(lr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n",
      "F1Score: 0.6880000000000001\n",
      "WeightedPrecision: 0.6811188811188811\n",
      "WeightedRecall: 0.7\n",
      "AUC: 0.5952380952380953\n"
     ]
    }
   ],
   "source": [
    "model_metrics(lr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: bigint, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_results.unpersist();  # clean up to free up space!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Random Forest Classifier Model\n"
     ]
    }
   ],
   "source": [
    "print('Baseline Random Forest Classifier Model')\n",
    "rfc_model = RandomForestClassifier().fit(train)\n",
    "rfc_results = rfc_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users predicted correctly: 33\n",
      "Total users predicted wrongly: 17\n",
      "Percentage predicted correct (%): 66.0 \n",
      "\n",
      "User churned and predicted to churn: 1\n",
      "Total users churned : 14\n",
      "Percent churned user events predicted correctly(%): 7.142857142857142\n",
      "\n",
      "User did not churn and predicted to: 4\n",
      "Percent churned users predicted incorrectly(%): 8.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_evaluation(rfc_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66\n",
      "F1Score: 0.5983625730994152\n",
      "WeightedPrecision: 0.5680000000000001\n",
      "WeightedRecall: 0.66\n",
      "AUC: 0.4801587301587301\n"
     ]
    }
   ],
   "source": [
    "model_metrics(rfc_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: bigint, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_results.unpersist()   # clean up to free up space!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline GBT Classifier Model\n"
     ]
    }
   ],
   "source": [
    "print('Baseline GBT Classifier Model')\n",
    "gbt_model = GBTClassifier().fit(train)\n",
    "gbt_results = gbt_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(7, {0: 0.1441, 1: 0.2835, 2: 0.0555, 3: 0.1037, 4: 0.0528, 5: 0.1618, 6: 0.1986})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users predicted correctly: 28\n",
      "Total users predicted wrongly: 22\n",
      "Percentage predicted correct (%): 56.00000000000001 \n",
      "\n",
      "User churned and predicted to churn: 1\n",
      "Total users churned : 14\n",
      "Percent churned user events predicted correctly(%): 7.142857142857142\n",
      "\n",
      "User did not churn and predicted to: 9\n",
      "Percent churned users predicted incorrectly(%): 18.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_evaluation(gbt_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n",
      "F1Score: 0.5349122807017545\n",
      "WeightedPrecision: 0.514\n",
      "WeightedRecall: 0.56\n",
      "AUC: 0.41071428571428575\n"
     ]
    }
   ],
   "source": [
    "model_metrics(gbt_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: bigint, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_results.unpersist()  # clean up to free up space!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline, just running it on classifier no transformations\n",
    "gbt_cv_model = GBTClassifier()\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt_cv_model])\n",
    "\n",
    "# set up param grid to iterate over\n",
    "paramGrid = ParamGridBuilder()\\\n",
    ".addGrid(gbt_model.maxDepth, [4, 6])\\\n",
    ".addGrid(gbt_model.maxBins, [15, 40])\\\n",
    ".addGrid(gbt_model.stepSize, [0.05, 0.2])\\\n",
    ".build()\n",
    "\n",
    "# default: 5\n",
    "# default: 32\n",
    "# default: 0.1\n",
    "\n",
    "# set up crossvalidator to tune parameters and optimize, returns best model\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                         estimatorParamMaps=paramGrid,\n",
    "                         evaluator=MulticlassClassificationEvaluator(predictionCol='prediction',\n",
    "                                                                     labelCol='label',\n",
    "                                                                     metricName='f1'),\n",
    "                         numFolds=3) #3\n",
    "\n",
    "#https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce\n",
    "#Recall= True_Positive/ (True_Positive+ False_Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(train)  # train model\n",
    "cv_results = cvModel.transform(test)  # apply model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_evaluation(cv_results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
