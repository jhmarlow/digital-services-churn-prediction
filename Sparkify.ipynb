{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Predicting User Churn in Digital Music Services"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Notebook to document data exploration and development of ML algorithm to identify at risk customers in digital music services."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Data Definition\n\nFrom Exploratory Data Analysis (EDA): \n#### Useful:\n- *location*: location of user, seems to append each new state (location, state)\n- *gender*: user gender (M/F/None)\n\n- *page*: what page the user is on during event (pages)\n- *level*: subscription level check uniqueness (free or paid)\n- *auth*: authenication (logged in/out)\n- *length*: time spent on page, max 50 mins on NextSong (if song paused??)\n\n- *registration*: unknown (registration unixtime)\n- *ts*: timestamp of event in ms (event unixtime)\n\n- *userId*: unique (userId val)\n- *sessionId*: unique sessionId per user?\n- *itemInSession*: lcounter for the number of items in a single session (item listened to in session)\n\n\n#### Not Useful:\n- *firstName*: users first name (not important, remove)\n- *lastName*: users lastname\n- *artist*: song artist\n- *song*: songname\n- *userAgent*: device/browser (not important for us, remove)\n- *method*: API PUT/GET http request (not important for us, remove)\n- *status*: http status"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Apache Spark on IBM Watson Setup"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Imports"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200626194806-0003\nKERNEL_ID = 71c997e1-caf0-4c6b-8802-8bb377aaf082\n"
                }
            ],
            "source": "# imports\nimport ibmos2spark\n\n# pyspark sql\nfrom pyspark.sql import SparkSession, Window\nfrom pyspark.sql.functions import from_unixtime, udf, col, when, isnan, desc\nfrom pyspark.sql.functions import sum as Fsum\nfrom pyspark.sql.types import IntegerType, StringType\nfrom pyspark.sql import functions as F\n\n# pyspark ml\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.pipeline import Pipeline\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# python\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### setup"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Build Spark session\nspark = SparkSession.builder.appName(\"User Churn\") .getOrCreate()\n\n# Read in data from IBM Cloud\ndata_df = spark.read.json(cos.url('medium-sparkify-event-data.json', 'sparkify-donotdelete-pr-fnqu5byx41gcai'))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Exploratory Data Analysis"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "data_df.printSchema()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "data_df.head(1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "data_df.toPandas().describe(include='all')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# ..."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Exploratory Data Analysis (EDA) -  using pysparksql"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# create temp sql table to explore data\ndata_df.createOrReplaceTempView(\"user_log_table\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Metadata: No. of Users in data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# how many users in the dataset, unique userId\nspark.sql(\"SELECT COUNT(DISTINCT(userId)) FROM user_log_table LIMIT 10\").show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Feature: Types of Pages"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "#\u00a0look at unique pages\nspark.sql(\"SELECT DISTINCT(page) FROM user_log_table LIMIT 100\").collect()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "From here we can see we want to identifying at risk customers by prediciting:\n- Cancel\n- Submit Downgrade\n- Downgrade\n- Cancellation Confirmation\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Feature: Types of level"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# unique levels\nspark.sql(\"SELECT DISTINCT(level) FROM user_log_table LIMIT 100\").collect()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Feature: authentication levels "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "spark.sql(\"SELECT DISTINCT(auth) FROM user_log_table LIMIT 100\").collect()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Feature: User Locations"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "spark.sql(\"SELECT DISTINCT(location) FROM user_log_table LIMIT 1000\").collect()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#                               ..."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Data Wrangling"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Remove non-useful columns and drop missing values"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def clean_df(user_log_valid):\n    \"\"\"Remove non useful data.\n    \"\"\"\n    # lets remove some of the columns we don't think will be useful from data exploration\n    cols_to_drop = ['firstName', 'lastName','artist', 'song', 'method', 'status', 'userAgent']\n    user_log_df = data_df.drop(*cols_to_drop)\n    # drop rows with missing info\n    user_log_valid = user_log_df.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])\n    \nuser_log_valid = clean_df(user_log_valid)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Convert UNIX timestamps to Datatime"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def unix_to_datetime(user_log_valid):\n    \"\"\" Convert unix timestamps to datetime\n    \"\"\"\n    # event unix to datetime\n    user_log_valid = user_log_valid.withColumn(\"timestamp_datetime\",\n                                         from_unixtime(user_log_valid.ts/1000,\n                                                       format='yyyy-MM-dd HH:mm:ss'))\n    # registration unix to datetime\n    user_log_valid = user_log_valid.withColumn(\"registration_datetime\",\n                                         from_unixtime(user_log_valid.registration/1000,\n                                                       format='yyyy-MM-dd HH:mm:ss'))\n    return user_log_valid\n\n\nuser_log_valid = unix_to_datetime(user_log_valid)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Creating US State Feature for Visualisation"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# missing values cause issue with split\nuser_log_valid.filter((user_log_df[\"location\"].isNull())).count()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def create_us_states(user_log_valid):\n    \"\"\"Create US states column from location\n    \"\"\"\n    #\u00a0we don't really want to drop these rows as the col isn't vital \n    # so replace missing values to allow split\n    user_log_valid = user_log_valid.fillna({'location':''})\n    # create state column\n    loc_split = udf(lambda x: x.split(', ')[-1], StringType())\n    # Sates seem to be appended, so take latest\n    state_split = udf(lambda x: x.split('-')[-1], StringType())\n\n    # apply udfs\n    user_log_valid = user_log_valid.withColumn(\"usstate_abbr\",\n                                         when(user_log_valid.location.isNotNull(),\n                                              loc_split(user_log_valid.location)).otherwise(''))\n    user_log_valid = user_log_valid.withColumn(\"usstate_abbr\",\n                                         when(user_log_valid.usstate_abbr.isNotNull(),\n                                              state_split(user_log_valid.usstate_abbr)).otherwise(''))\n    \nuser_log_valid = create_us_states(user_log_valid)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# take a look\nuser_log_valid.head(1)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# ML Feature Engineering"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Flag user Cancellations and Create Phase"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def create_phase_feature(user_log_valid):\n    \"\"\"Use the cancellation to identify churned users.\n    \"\"\"\n    flag_cancellation_event = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n    user_log_valid = user_log_valid.withColumn(\"churn\", flag_cancellation_event(\"page\"))\n    windowval = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n    user_log_valid = user_log_valid.withColumn(\"label\", Fsum(\"churn\").over(windowval))\n    \nuser_log_valid = create_phase_feature(user_log_valid)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "user_log_valid.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "user_log_valid.filter(user_log_valid['userId']==100010).head(50000)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Calculate Hours Since Registration"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def hours_since_reg(user_log_valid):\n\n    # hours since registration\n    user_log_valid = user_log_valid.withColumn('hours_since_registration',\n                                         (user_log_valid['ts'] - user_log_valid['registration']) / (1000 *3600))\n    return user_log_valid.withColumn(\"hours_since_registration\", user_log_valid[\"hours_since_registration\"].cast(IntegerType()))\n\nuser_log_valid = hours_since_reg(user_log_valid)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Calculate Hour in the Day of Event"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def hour_in_day(user_log_valid):\n\n    # hour in the day of event\n    get_hour = udf(lambda x:  int(datetime.datetime.fromtimestamp(x / 1000.0).hour)) \n    user_log_valid = user_log_valid.withColumn(\"hour\", get_hour(user_log_valid.ts))\n    return user_log_valid\n\nuser_log_valid = hour_in_day(user_log_valid)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false
            },
            "outputs": [],
            "source": "def avg_user_items_in_sesh(user_log_valid):\n    # calculate average listening time\n    windowval = Window.partitionBy(\"userId\").orderBy(\"ts\").rangeBetween(Window.unboundedPreceding, 0)\n    return user_log_valid.withColumn('itemInSession_rolling_average', F.avg(\"itemInSession\").over(windowval))\n    \nuser_log_valid = avg_user_items_in_sesh(user_log_valid)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def avg_user_listening_time(user_log_valid)\n    # calculate average listening time\n    windowval = Window.partitionBy(\"userId\").orderBy(\"ts\").rangeBetween(Window.unboundedPreceding, 0)\n    return user_log_valid.withColumn('length_rolling_average', F.avg(\"length\").over(windowval))\n\nuser_log_valid = avg_user_listening_time(user_log_valid)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "user_log_valid.filter(user_log_valid['userId']==293).select(\"sessionId\",\"length\",\"length_rolling_average\").head(5)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def num_neg_user_events(user_log_valid):\n    # Number of Positive Events\n    return user_log_valid.withColumn(\"positive_event\",\n                                         when((user_log_valid[\"page\"] == 'Add to Playlist') |\\\n                                              (user_log_valid[\"page\"] == 'Add Friend') |\\\n                                              (user_log_valid[\"page\"] == 'Thumbs Up'),\n                                              1).otherwise(0))\n\nuser_log_valid = num_neg_user_events(user_log_valid)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def num_pos_user_events(user_log_valid):\n    # Number of Negative Events\n    return user_log_valid.withColumn(\"negative_event\",\n                                         when((user_log_valid[\"page\"] == 'Thumbs Down') |\\\n                                              (user_log_valid[\"page\"] == 'Help') |\\\n                                              (user_log_valid[\"page\"] == 'Error'),\n                                              1).otherwise(0))\n\nuser_log_valid = num_pos_user_events(user_log_valid)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "user_log_valid.head(1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "pd_features = features_df.toPandas()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "fig = plt.figure(figsize=(30,25))\nax = fig.gca()\nh = pd_features.hist(ax=ax)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Data Setup for ML Algorithm"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "ml_df_prep=user_log_valid"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Create Features"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Onehot Encode Categorical Variables"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "## https://stackoverflow.com/questions/32277576/how-to-handle-categorical-features-with-spark-ml\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n\nindexer = StringIndexer(inputCol=\"level\", outputCol=\"levelIndex\")\ninputs = [indexer.getOutputCol()]\nencoder = OneHotEncoderEstimator(inputCols=inputs, outputCols=[\"levelVec\"])\n\npipeline = Pipeline(stages=[indexer, encoder])\nml_df_prep = pipeline.fit(ml_df_prep).transform(ml_df_prep)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Create Features Vector"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# this vector is created in prep for ml\nassembler = VectorAssembler(inputCols=[\"sessionId\",\n                                       \"itemInSession\",\n                                       \"hours_since_registration\",\n                                       \"levelVec\"],\n                            outputCol=\"features\",\n                           handleInvalid=\"skip\")\nml_df_prep = assembler.transform(ml_df_prep)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#\u00a0apply scaler\nscaler = Normalizer(inputCol=\"features\", outputCol=\"ScaledFeatures\")\nml_df_prep = scaler.transform(ml_df_prep)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "ml_df_prep.head(1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "ml_df = ml_df_prep.select(\"label\",\"features\")\nml_df.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Train ML Model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# train test split for ML validation\ntrain, test =  ml_df.randomSplit([0.8, 0.2], seed=42)  #\u00a0more equal fit to combat overfitting\ntrain.head(50)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "train.filter(train['label']==1).count()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "train.filter(train['label']==0).count()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "train.filter(train['label']==1).count()/train.filter(train['label']==0).count()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# estimators\nlr = LogisticRegression(maxIter=10, regParam=0.0, elasticNetParam=0)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Baseline"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "baseline binary Logisitc Regression Model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "lrmodel = lr.fit(train)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "lr_results = lrmodel.transform(test) "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "lr_results.filter(lr_results[\"prediction\"]==1).head(10)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "lrmodel.summary.accuracy"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "lrmodel.summary.fMeasureByLabel()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "lrmodel.summary.precisionByLabel"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Optimised"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# pipeline, just running it on classifier no transformations\npipeline = Pipeline(stages=[lr])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# set up param grid to iterate over\nparamGrid = ParamGridBuilder() \\\n.addGrid(lr.regParam, [0.0, 0.1]) \\\n.build()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# set up crossvalidator to tune parameters and optimize\ncrossval = CrossValidator(estimator=pipeline,\n                         estimatorParamMaps=paramGrid,\n                         evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n                         numFolds=2)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "cvModel = crossval.fit(train)  # train model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "results = cvModel.transform(test)  # apply model on test data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "cvModel.avgMetrics  # look at model scoring metrics"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "results.count()  # how many events in total labels"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(results.filter(results.label == results.prediction).count())  # check how many were predicted correctly"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "results.filter(results.label == results.prediction).count()/results.count()   # hwow many correct"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "results.filter(results[\"prediction\"]==1).head(5)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "results.filter(results[\"label\"]==1).head(50)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}