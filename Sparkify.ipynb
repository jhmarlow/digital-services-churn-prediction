{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Predicting User Churn in Digital Music Services"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Notebook to document data exploration and development of ML algorithm to identify at risk customers in digital music services."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Data Definition\n\nFrom Exploratory Data Analysis (EDA): \n#### Useful:\n- *location*: location of user, seems to append each new state (location, state)\n- *gender*: user gender (M/F/None)\n\n- *page*: what page the user is on during event (pages)\n- *level*: subscription level check uniqueness (free or paid)\n- *auth*: authenication (logged in/out)\n- *length*: time spent on page, max 50 mins on NextSong (if song paused??)\n\n- *registration*: unknown (registration unixtime)\n- *ts*: timestamp of event in ms (event unixtime)\n\n- *userId*: unique (userId val)\n- *sessionId*: unique sessionId per user?\n- *itemInSession*: lcounter for the number of items in a single session (item listened to in session)\n\n\n#### Not Useful:\n- *firstName*: users first name (not important, remove)\n- *lastName*: users lastname\n- *artist*: song artist\n- *song*: songname\n- *userAgent*: device/browser (not important for us, remove)\n- *method*: API PUT/GET http request (not important for us, remove)\n- *status*: http status"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Apache Spark on IBM Watson Setup"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200621184655-0005\nKERNEL_ID = 4e3e8129-b3d1-4f06-94f0-3ea706b93dd4\n"
                }
            ],
            "source": "# imports\nimport ibmos2spark\n\n# pyspark sql\nfrom pyspark.sql import SparkSession, Window\nfrom pyspark.sql.functions import from_unixtime, udf, col, when, isnan, desc\nfrom pyspark.sql.functions import sum as Fsum\nfrom pyspark.sql.types import IntegerType, StringType\n\n# pyspark ml\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.pipeline import Pipeline\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# python\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "# Build Spark session\nspark = SparkSession.builder.appName(\"User Churn\") .getOrCreate()\n\n# Read in data from IBM Cloud\ndata_df = spark.read.json(cos.url('medium-sparkify-event-data.json', 'sparkify-donotdelete-pr-fnqu5byx41gcai'))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Exploratory Data Analysis"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "root\n |-- artist: string (nullable = true)\n |-- auth: string (nullable = true)\n |-- firstName: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- itemInSession: long (nullable = true)\n |-- lastName: string (nullable = true)\n |-- length: double (nullable = true)\n |-- level: string (nullable = true)\n |-- location: string (nullable = true)\n |-- method: string (nullable = true)\n |-- page: string (nullable = true)\n |-- registration: long (nullable = true)\n |-- sessionId: long (nullable = true)\n |-- song: string (nullable = true)\n |-- status: long (nullable = true)\n |-- ts: long (nullable = true)\n |-- userAgent: string (nullable = true)\n |-- userId: string (nullable = true)\n\n"
                }
            ],
            "source": "data_df.printSchema()"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Row(artist='Martin Orford', auth='Logged In', firstName='Joseph', gender='M', itemInSession=20, lastName='Morales', length=597.55057, level='free', location='Corpus Christi, TX', method='PUT', page='NextSong', registration=1532063507000, sessionId=292, song='Grand Designs', status=200, ts=1538352011000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='293')]"
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "data_df.head(1)"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist</th>\n      <th>auth</th>\n      <th>firstName</th>\n      <th>gender</th>\n      <th>itemInSession</th>\n      <th>lastName</th>\n      <th>length</th>\n      <th>level</th>\n      <th>location</th>\n      <th>method</th>\n      <th>page</th>\n      <th>registration</th>\n      <th>sessionId</th>\n      <th>song</th>\n      <th>status</th>\n      <th>ts</th>\n      <th>userAgent</th>\n      <th>userId</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>432877</td>\n      <td>543705</td>\n      <td>528005</td>\n      <td>528005</td>\n      <td>543705.000000</td>\n      <td>528005</td>\n      <td>432877.000000</td>\n      <td>543705</td>\n      <td>528005</td>\n      <td>543705</td>\n      <td>543705</td>\n      <td>5.280050e+05</td>\n      <td>543705.000000</td>\n      <td>432877</td>\n      <td>543705.000000</td>\n      <td>5.437050e+05</td>\n      <td>528005</td>\n      <td>543705</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>21247</td>\n      <td>4</td>\n      <td>345</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>275</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>192</td>\n      <td>2</td>\n      <td>22</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>80292</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>71</td>\n      <td>449</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Kings Of Leon</td>\n      <td>Logged In</td>\n      <td>Joseph</td>\n      <td>M</td>\n      <td>NaN</td>\n      <td>Reed</td>\n      <td>NaN</td>\n      <td>paid</td>\n      <td>New York-Newark-Jersey City, NY-NJ-PA</td>\n      <td>PUT</td>\n      <td>NextSong</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>You're The One</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>3497</td>\n      <td>527906</td>\n      <td>13108</td>\n      <td>302612</td>\n      <td>NaN</td>\n      <td>12767</td>\n      <td>NaN</td>\n      <td>428597</td>\n      <td>40156</td>\n      <td>495143</td>\n      <td>432877</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2219</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>46082</td>\n      <td>15700</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>107.306291</td>\n      <td>NaN</td>\n      <td>248.664593</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.535523e+12</td>\n      <td>2040.814353</td>\n      <td>NaN</td>\n      <td>210.018291</td>\n      <td>1.540965e+12</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>116.723508</td>\n      <td>NaN</td>\n      <td>98.412670</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.078725e+09</td>\n      <td>1434.338931</td>\n      <td>NaN</td>\n      <td>31.471919</td>\n      <td>1.482057e+09</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.783220</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.509854e+12</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>200.000000</td>\n      <td>1.538352e+12</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>26.000000</td>\n      <td>NaN</td>\n      <td>199.392200</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.534368e+12</td>\n      <td>630.000000</td>\n      <td>NaN</td>\n      <td>200.000000</td>\n      <td>1.539720e+12</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>68.000000</td>\n      <td>NaN</td>\n      <td>234.004440</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.536556e+12</td>\n      <td>1968.000000</td>\n      <td>NaN</td>\n      <td>200.000000</td>\n      <td>1.541005e+12</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>147.000000</td>\n      <td>NaN</td>\n      <td>276.793020</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.537612e+12</td>\n      <td>3307.000000</td>\n      <td>NaN</td>\n      <td>200.000000</td>\n      <td>1.542177e+12</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1005.000000</td>\n      <td>NaN</td>\n      <td>3024.665670</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.543074e+12</td>\n      <td>4808.000000</td>\n      <td>NaN</td>\n      <td>404.000000</td>\n      <td>1.543622e+12</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "               artist       auth firstName  gender  itemInSession lastName  \\\ncount          432877     543705    528005  528005  543705.000000   528005   \nunique          21247          4       345       2            NaN      275   \ntop     Kings Of Leon  Logged In    Joseph       M            NaN     Reed   \nfreq             3497     527906     13108  302612            NaN    12767   \nmean              NaN        NaN       NaN     NaN     107.306291      NaN   \nstd               NaN        NaN       NaN     NaN     116.723508      NaN   \nmin               NaN        NaN       NaN     NaN       0.000000      NaN   \n25%               NaN        NaN       NaN     NaN      26.000000      NaN   \n50%               NaN        NaN       NaN     NaN      68.000000      NaN   \n75%               NaN        NaN       NaN     NaN     147.000000      NaN   \nmax               NaN        NaN       NaN     NaN    1005.000000      NaN   \n\n               length   level                               location  method  \\\ncount   432877.000000  543705                                 528005  543705   \nunique            NaN       2                                    192       2   \ntop               NaN    paid  New York-Newark-Jersey City, NY-NJ-PA     PUT   \nfreq              NaN  428597                                  40156  495143   \nmean       248.664593     NaN                                    NaN     NaN   \nstd         98.412670     NaN                                    NaN     NaN   \nmin          0.783220     NaN                                    NaN     NaN   \n25%        199.392200     NaN                                    NaN     NaN   \n50%        234.004440     NaN                                    NaN     NaN   \n75%        276.793020     NaN                                    NaN     NaN   \nmax       3024.665670     NaN                                    NaN     NaN   \n\n            page  registration      sessionId            song         status  \\\ncount     543705  5.280050e+05  543705.000000          432877  543705.000000   \nunique        22           NaN            NaN           80292            NaN   \ntop     NextSong           NaN            NaN  You're The One            NaN   \nfreq      432877           NaN            NaN            2219            NaN   \nmean         NaN  1.535523e+12    2040.814353             NaN     210.018291   \nstd          NaN  3.078725e+09    1434.338931             NaN      31.471919   \nmin          NaN  1.509854e+12       1.000000             NaN     200.000000   \n25%          NaN  1.534368e+12     630.000000             NaN     200.000000   \n50%          NaN  1.536556e+12    1968.000000             NaN     200.000000   \n75%          NaN  1.537612e+12    3307.000000             NaN     200.000000   \nmax          NaN  1.543074e+12    4808.000000             NaN     404.000000   \n\n                  ts                                          userAgent  \\\ncount   5.437050e+05                                             528005   \nunique           NaN                                                 71   \ntop              NaN  \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...   \nfreq             NaN                                              46082   \nmean    1.540965e+12                                                NaN   \nstd     1.482057e+09                                                NaN   \nmin     1.538352e+12                                                NaN   \n25%     1.539720e+12                                                NaN   \n50%     1.541005e+12                                                NaN   \n75%     1.542177e+12                                                NaN   \nmax     1.543622e+12                                                NaN   \n\n        userId  \ncount   543705  \nunique     449  \ntop             \nfreq     15700  \nmean       NaN  \nstd        NaN  \nmin        NaN  \n25%        NaN  \n50%        NaN  \n75%        NaN  \nmax        NaN  "
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "data_df.toPandas().describe(include='all')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# ..."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Exploratory Data Analysis (EDA) -  using pysparksql"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "# create temp sql table to explore data\ndata_df.createOrReplaceTempView(\"user_log_table\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Metadata: No. of Users in data"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+----------------------+\n|count(DISTINCT userId)|\n+----------------------+\n|                   449|\n+----------------------+\n\n"
                }
            ],
            "source": "# how many users in the dataset, unique userId\nspark.sql(\"SELECT COUNT(DISTINCT(userId)) FROM user_log_table LIMIT 10\").show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Feature: Types of Pages"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Row(page='Cancel'),\n Row(page='Submit Downgrade'),\n Row(page='Thumbs Down'),\n Row(page='Home'),\n Row(page='Downgrade'),\n Row(page='Roll Advert'),\n Row(page='Logout'),\n Row(page='Save Settings'),\n Row(page='Cancellation Confirmation'),\n Row(page='About'),\n Row(page='Submit Registration'),\n Row(page='Settings'),\n Row(page='Login'),\n Row(page='Register'),\n Row(page='Add to Playlist'),\n Row(page='Add Friend'),\n Row(page='NextSong'),\n Row(page='Thumbs Up'),\n Row(page='Help'),\n Row(page='Upgrade'),\n Row(page='Error'),\n Row(page='Submit Upgrade')]"
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "#\u00a0look at unique pages\nspark.sql(\"SELECT DISTINCT(page) FROM user_log_table LIMIT 100\").collect()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "From here we can see we want to identifying at risk customers by prediciting:\n- Cancel\n- Submit Downgrade\n- Downgrade\n- Cancellation Confirmation\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Feature: Types of level"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Row(level='free'), Row(level='paid')]"
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# unique levels\nspark.sql(\"SELECT DISTINCT(level) FROM user_log_table LIMIT 100\").collect()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Feature: authentication levels "
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Row(auth='Logged Out'),\n Row(auth='Cancelled'),\n Row(auth='Guest'),\n Row(auth='Logged In')]"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "spark.sql(\"SELECT DISTINCT(auth) FROM user_log_table LIMIT 100\").collect()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Feature: User Locations"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Row(location='Atlantic City-Hammonton, NJ'),\n Row(location='Gainesville, FL'),\n Row(location='Richmond, VA'),\n Row(location='Oskaloosa, IA'),\n Row(location='Tucson, AZ'),\n Row(location='Deltona-Daytona Beach-Ormond Beach, FL'),\n Row(location='San Diego-Carlsbad, CA'),\n Row(location='Cleveland-Elyria, OH'),\n Row(location='Medford, OR'),\n Row(location='Kingsport-Bristol-Bristol, TN-VA'),\n Row(location='New Haven-Milford, CT'),\n Row(location='Birmingham-Hoover, AL'),\n Row(location='Corpus Christi, TX'),\n Row(location='Mobile, AL'),\n Row(location='Dubuque, IA'),\n Row(location='Las Vegas-Henderson-Paradise, NV'),\n Row(location='Killeen-Temple, TX'),\n Row(location='Ottawa-Peru, IL'),\n Row(location='Boise City, ID'),\n Row(location='Bremerton-Silverdale, WA'),\n Row(location='Urban Honolulu, HI'),\n Row(location='Cedar City, UT'),\n Row(location='Indianapolis-Carmel-Anderson, IN'),\n Row(location='Durham-Chapel Hill, NC'),\n Row(location='Seattle-Tacoma-Bellevue, WA'),\n Row(location='Fort Smith, AR-OK'),\n Row(location='Champaign-Urbana, IL'),\n Row(location='Columbus, OH'),\n Row(location='Albany, OR'),\n Row(location='Winston-Salem, NC'),\n Row(location='Los Angeles-Long Beach-Anaheim, CA'),\n Row(location='Bakersfield, CA'),\n Row(location='Minneapolis-St. Paul-Bloomington, MN-WI'),\n Row(location='San Francisco-Oakland-Hayward, CA'),\n Row(location='Tyler, TX'),\n Row(location='Lafayette-West Lafayette, IN'),\n Row(location='Phoenix-Mesa-Scottsdale, AZ'),\n Row(location='Allentown-Bethlehem-Easton, PA-NJ'),\n Row(location='Miami-Fort Lauderdale-West Palm Beach, FL'),\n Row(location='Topeka, KS'),\n Row(location='Selma, AL'),\n Row(location='Lebanon, PA'),\n Row(location='Dayton, OH'),\n Row(location='San Jose-Sunnyvale-Santa Clara, CA'),\n Row(location='Bridgeport-Stamford-Norwalk, CT'),\n Row(location='Providence-Warwick, RI-MA'),\n Row(location='Sioux Falls, SD'),\n Row(location='Price, UT'),\n Row(location='Greenville-Anderson-Mauldin, SC'),\n Row(location='Columbus, GA-AL'),\n Row(location='Akron, OH'),\n Row(location='Racine, WI'),\n Row(location='Logan, UT-ID'),\n Row(location='Myrtle Beach-Conway-North Myrtle Beach, SC-NC'),\n Row(location=None),\n Row(location='Provo-Orem, UT'),\n Row(location='Buffalo-Cheektowaga-Niagara Falls, NY'),\n Row(location='Cookeville, TN'),\n Row(location='Big Spring, TX'),\n Row(location='New Philadelphia-Dover, OH'),\n Row(location='York-Hanover, PA'),\n Row(location='Beaumont-Port Arthur, TX'),\n Row(location='Anchorage, AK'),\n Row(location='Tulsa, OK'),\n Row(location='New York-Newark-Jersey City, NY-NJ-PA'),\n Row(location='Danville, IL'),\n Row(location='Charleston, WV'),\n Row(location='St. Louis, MO-IL'),\n Row(location='Janesville-Beloit, WI'),\n Row(location='Orlando-Kissimmee-Sanford, FL'),\n Row(location='Jackson, MS'),\n Row(location='Rockingham, NC'),\n Row(location='Gallup, NM'),\n Row(location='Alexandria, MN'),\n Row(location='McAllen-Edinburg-Mission, TX'),\n Row(location='Oxnard-Thousand Oaks-Ventura, CA'),\n Row(location='Memphis, TN-MS-AR'),\n Row(location='Vermillion, SD'),\n Row(location='Milledgeville, GA'),\n Row(location='Auburn, IN'),\n Row(location='San Antonio-New Braunfels, TX'),\n Row(location='New Orleans-Metairie, LA'),\n Row(location='Troy, AL'),\n Row(location='Duluth, MN-WI'),\n Row(location='Albemarle, NC'),\n Row(location='Ozark, AL'),\n Row(location='Danville, VA'),\n Row(location='Washington-Arlington-Alexandria, DC-VA-MD-WV'),\n Row(location='Greensboro-High Point, NC'),\n Row(location='Manchester-Nashua, NH'),\n Row(location='Sterling, IL'),\n Row(location='Rio Grande City, TX'),\n Row(location='Napa, CA'),\n Row(location='Monroe, LA'),\n Row(location='Knoxville, TN'),\n Row(location='Louisville/Jefferson County, KY-IN'),\n Row(location='Norwich-New London, CT'),\n Row(location='Nashville-Davidson--Murfreesboro--Franklin, TN'),\n Row(location='El Paso, TX'),\n Row(location='Syracuse, NY'),\n Row(location='Marion, IN'),\n Row(location='Little Rock-North Little Rock-Conway, AR'),\n Row(location='Austin-Round Rock, TX'),\n Row(location='Toledo, OH'),\n Row(location='El Campo, TX'),\n Row(location='Kansas City, MO-KS'),\n Row(location='Cincinnati, OH-KY-IN'),\n Row(location='Valdosta, GA'),\n Row(location='Cullman, AL'),\n Row(location='Bozeman, MT'),\n Row(location='Truckee-Grass Valley, CA'),\n Row(location='Grand Rapids-Wyoming, MI'),\n Row(location='Boston-Cambridge-Newton, MA-NH'),\n Row(location='Philadelphia-Camden-Wilmington, PA-NJ-DE-MD'),\n Row(location='Atlanta-Sandy Springs-Roswell, GA'),\n Row(location='Klamath Falls, OR'),\n Row(location='Pittsburgh, PA'),\n Row(location='Las Vegas, NM'),\n Row(location='Bowling Green, KY'),\n Row(location='Rochester, MN'),\n Row(location='Chicago-Naperville-Elgin, IL-IN-WI'),\n Row(location='Dallas-Fort Worth-Arlington, TX'),\n Row(location='Mansfield, OH'),\n Row(location='North Wilkesboro, NC'),\n Row(location='Oklahoma City, OK'),\n Row(location='Albany-Schenectady-Troy, NY'),\n Row(location='Quincy, IL-MO'),\n Row(location='Houghton, MI'),\n Row(location='Tullahoma-Manchester, TN'),\n Row(location='Pontiac, IL'),\n Row(location='Worcester, MA-CT'),\n Row(location='Jacksonville, FL'),\n Row(location='Fairbanks, AK'),\n Row(location='Fresno, CA'),\n Row(location='Houston-The Woodlands-Sugar Land, TX'),\n Row(location='Lafayette, LA'),\n Row(location='Lakeland-Winter Haven, FL'),\n Row(location='Decatur, AL'),\n Row(location='Brunswick, GA'),\n Row(location='Montgomery, AL'),\n Row(location='Hartford-West Hartford-East Hartford, CT'),\n Row(location='Port St. Lucie, FL'),\n Row(location='Omaha-Council Bluffs, NE-IA'),\n Row(location='Rutland, VT'),\n Row(location='Charlotte-Concord-Gastonia, NC-SC'),\n Row(location='Madison, WI'),\n Row(location='Baltimore-Columbia-Towson, MD'),\n Row(location='Raleigh, NC'),\n Row(location='Laurel, MS'),\n Row(location='Sacramento--Roseville--Arden-Arcade, CA'),\n Row(location='Lexington-Fayette, KY'),\n Row(location='Tampa-St. Petersburg-Clearwater, FL'),\n Row(location='Modesto, CA'),\n Row(location='London, KY'),\n Row(location='Tallahassee, FL'),\n Row(location='Roanoke, VA'),\n Row(location='Virginia Beach-Norfolk-Newport News, VA-NC'),\n Row(location='Riverside-San Bernardino-Ontario, CA'),\n Row(location='Appleton, WI'),\n Row(location='Concord, NH'),\n Row(location='Rochester, NY'),\n Row(location='Fargo, ND-MN'),\n Row(location='Midland, MI'),\n Row(location='Stockton-Lodi, CA'),\n Row(location='Boulder, CO'),\n Row(location='Milwaukee-Waukesha-West Allis, WI'),\n Row(location='Detroit-Warren-Dearborn, MI'),\n Row(location='Billings, MT'),\n Row(location='Alexandria, LA'),\n Row(location='Salinas, CA'),\n Row(location='Ionia, MI'),\n Row(location='Vineland-Bridgeton, NJ'),\n Row(location='Morgantown, WV'),\n Row(location='Show Low, AZ'),\n Row(location='Jacksonville, NC'),\n Row(location='Athens, TX'),\n Row(location='Augusta-Waterville, ME'),\n Row(location='Espa\u00f1ola, NM'),\n Row(location='Salt Lake City, UT'),\n Row(location='Wichita, KS'),\n Row(location='Spokane-Spokane Valley, WA'),\n Row(location='Fort Madison-Keokuk, IA-IL-MO'),\n Row(location='Kankakee, IL'),\n Row(location='Portland-Vancouver-Hillsboro, OR-WA'),\n Row(location='Newport, TN'),\n Row(location='Denver-Aurora-Lakewood, CO'),\n Row(location='Flint, MI'),\n Row(location='Santa Maria-Santa Barbara, CA'),\n Row(location='Muncie, IN'),\n Row(location='Fort Wayne, IN'),\n Row(location='Colorado Springs, CO'),\n Row(location='Wilson, NC'),\n Row(location='Hagerstown-Martinsburg, MD-WV')]"
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "spark.sql(\"SELECT DISTINCT(location) FROM user_log_table LIMIT 1000\").collect()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#                               ..."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Data Wrangling"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Remove non-useful columns and drop missing values"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "# lets remove some of the columns we don't think will be useful from data exploration\ncols_to_drop = ['firstName', 'lastName','artist', 'song', 'method', 'status', 'userAgent']\nuser_log_df = data_df.drop(*cols_to_drop)"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": "# drop rows with missing info\nuser_log_valid = user_log_df.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Convert UNIX timestamps to Datatime"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "# event unix to datetime\nuser_log_valid = user_log_valid.withColumn(\"timestamp_datetime\",\n                                     from_unixtime(user_log_valid.ts/1000,\n                                                   format='yyyy-MM-dd HH:mm:ss'))"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "# registration unix to datetime\nuser_log_valid = user_log_valid.withColumn(\"registration_datetime\",\n                                     from_unixtime(user_log_valid.registration/1000,\n                                                   format='yyyy-MM-dd HH:mm:ss'))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Creating US State Feature for Visualisation"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "15700"
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# missing values cause issue with split\nuser_log_valid.filter((user_log_df[\"location\"].isNull())).count()"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": "#\u00a0we don't really want to drop these rows as the col isn't vital \n# so replace missing values to allow split\nuser_log_valid = user_log_valid.fillna({'location':''})\n# create state column\nloc_split = udf(lambda x: x.split(', ')[-1], StringType())\n# Sates seem to be appended, so take latest\nstate_split = udf(lambda x: x.split('-')[-1], StringType())\n\n# apply udfs\nuser_log_valid = user_log_valid.withColumn(\"usstate_abbr\",\n                                     when(user_log_valid.location.isNotNull(),\n                                          loc_split(user_log_valid.location)).otherwise(''))\nuser_log_valid = user_log_valid.withColumn(\"usstate_abbr\",\n                                     when(user_log_valid.usstate_abbr.isNotNull(),\n                                          state_split(user_log_valid.usstate_abbr)).otherwise(''))"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Row(auth='Logged In', gender='M', itemInSession=20, length=597.55057, level='free', location='Corpus Christi, TX', page='NextSong', registration=1532063507000, sessionId=292, ts=1538352011000, userId='293', timestamp_datetime='2018-10-01 00:00:11', registration_datetime='2018-07-20 05:11:47', usstate_abbr='TX')]"
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# take a look\nuser_log_valid.head(1)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Feature Engineering"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Flag user Downgrades and Create Phase"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": "flag_downgrade_event = udf(lambda x: 1 if x == \"Submit Downgrade\" else 0, IntegerType())\nuser_log_valid = user_log_valid.withColumn(\"downgraded\", flag_downgrade_event(\"page\"))\n\nwindowval = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n\nuser_log_valid = user_log_valid.withColumn(\"label\", Fsum(\"downgraded\").over(windowval))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Calculate Hours Since Registration"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": "# hours since registration\nuser_log_valid = user_log_valid.withColumn('hours_since_registration',\n                                     (user_log_valid['ts'] - user_log_valid['registration']) / (1000 *3600))\nuser_log_valid = user_log_valid.withColumn(\"hours_since_registration\", user_log_valid[\"hours_since_registration\"].cast(IntegerType()))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Calculate Hour in the Day of Event"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": "# hour in the day of event\nget_hour = udf(lambda x:  int(datetime.datetime.fromtimestamp(x / 1000.0).hour)) \nuser_log_valid = user_log_valid.withColumn(\"hour\", get_hour(user_log_valid.ts))"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {
                "scrolled": false
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Row(sessionId=292, itemInSession=20, itemInSession_rolling_average=20.0),\n Row(sessionId=292, itemInSession=21, itemInSession_rolling_average=20.5),\n Row(sessionId=292, itemInSession=22, itemInSession_rolling_average=21.0),\n Row(sessionId=292, itemInSession=23, itemInSession_rolling_average=21.5),\n Row(sessionId=292, itemInSession=24, itemInSession_rolling_average=22.0)]"
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from pyspark.sql import functions as F\n# calculate average listening time\nwindowval = Window.partitionBy(\"userId\").orderBy(\"ts\").rangeBetween(Window.unboundedPreceding, 0)\nuser_log_valid = user_log_valid.withColumn('itemInSession_rolling_average', F.avg(\"itemInSession\").over(windowval))\nuser_log_valid.filter(user_log_valid['userId']==293).select(\"sessionId\",\"itemInSession\",\"itemInSession_rolling_average\").head(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Row(sessionId=292, length=597.55057, length_rolling_average=597.55057),\n Row(sessionId=292, length=180.50567, length_rolling_average=389.02812),\n Row(sessionId=292, length=268.59057, length_rolling_average=348.88227),\n Row(sessionId=292, length=None, length_rolling_average=348.88227),\n Row(sessionId=292, length=232.88118, length_rolling_average=319.8819975)]"
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# calculate average listening time\nwindowval = Window.partitionBy(\"userId\").orderBy(\"ts\").rangeBetween(Window.unboundedPreceding, 0)\nuser_log_valid = user_log_valid.withColumn('length_rolling_average', F.avg(\"length\").over(windowval))\nuser_log_valid.filter(user_log_valid['userId']==293).select(\"sessionId\",\"length\",\"length_rolling_average\").head(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": "# Number of Positive Events\nuser_log_valid = user_log_valid.withColumn(\"positive_event\",\n                                     when((user_log_valid[\"page\"] == 'Add to Playlist') |\\\n                                          (user_log_valid[\"page\"] == 'Add Friend') |\\\n                                          (user_log_valid[\"page\"] == 'Thumbs Up'),\n                                          1).otherwise(0))"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": "# Number of Negative Events\nuser_log_valid = user_log_valid.withColumn(\"negative_event\",\n                                     when((user_log_valid[\"page\"] == 'Thumbs Down') |\\\n                                          (user_log_valid[\"page\"] == 'Help') |\\\n                                          (user_log_valid[\"page\"] == 'Error'),\n                                          1).otherwise(0))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Data Setup for ML Algorithm"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": "ml_df_prep =user_log_valid"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "107526"
                    },
                    "execution_count": 29,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# how many churn events in dataset\nml_df_prep.filter(ml_df_prep[\"label\"]==1).count()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Create Features"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Onehot Encode Categorical Variables"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": "## https://stackoverflow.com/questions/32277576/how-to-handle-categorical-features-with-spark-ml\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n\nindexer = StringIndexer(inputCol=\"level\", outputCol=\"levelIndex\")\ninputs = [indexer.getOutputCol()]\nencoder = OneHotEncoderEstimator(inputCols=inputs, outputCols=[\"levelVec\"])\n\npipeline = Pipeline(stages=[indexer, encoder])\nml_df_prep = pipeline.fit(ml_df_prep).transform(ml_df_prep)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Create Features Vector"
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": "# this vector is created in prep for ml\nassembler = VectorAssembler(inputCols=[\"sessionId\",\n                                       \"itemInSession\",\n                                       \"hours_since_registration\",\n                                      \"levelVec\"],\n                            outputCol=\"features\",\n                           handleInvalid=\"skip\")\nml_df_prep = assembler.transform(ml_df_prep)"
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": "#\u00a0apply scaler\nscaler = Normalizer(inputCol=\"features\", outputCol=\"ScaledFeatures\")\nml_df_prep = scaler.transform(ml_df_prep)"
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Row(auth='Logged In', gender='F', itemInSession=0, length=226.08934, level='free', location='Bridgeport-Stamford-Norwalk, CT', page='NextSong', registration=1538016340000, sessionId=62, ts=1538991392000, userId='100010', timestamp_datetime='2018-10-08 09:36:32', registration_datetime='2018-09-27 02:45:40', usstate_abbr='CT', downgraded=0, label=0, hours_since_registration=270, hour='9', itemInSession_rolling_average=0.0, length_rolling_average=226.08934, positive_event=0, negative_event=0, levelIndex=1.0, levelVec=SparseVector(1, {}), features=DenseVector([62.0, 0.0, 270.0, 0.0]), ScaledFeatures=DenseVector([0.2238, 0.0, 0.9746, 0.0]))]"
                    },
                    "execution_count": 45,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "ml_df_prep.head(1)"
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "Row(label=0, features=DenseVector([166.0, 67.0, 343.0, 0.0]))"
                    },
                    "execution_count": 44,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "ml_df = ml_df_prep.select(\"label\",\"features\")\nml_df.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Train ML Model"
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Row(label=0, features=DenseVector([2.0, 4.0, 590.0, 0.0]))]"
                    },
                    "execution_count": 37,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# train test split for ML validation\ntrain, test =  ml_df.randomSplit([0.6, 0.4], seed=42)  #\u00a0more equal fit to combat overfitting\ntrain.head(1)"
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": "# estimators\nlr = LogisticRegression(maxIter=10, regParam=0.0, elasticNetParam=0)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Baseline"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "baseline binary Logisitc Regression Model"
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": "lrmodel = lr.fit(train)"
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": "lr_results = lrmodel.transform(test) "
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Row(label=1, features=DenseVector([477.0, 0.0, 1121.0, 0.0]), rawPrediction=DenseVector([2.7007, 1.4516, -0.8783, -3.274]), probability=DenseVector([0.7592, 0.2177, 0.0212, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 6.0, 1121.0, 0.0]), rawPrediction=DenseVector([2.705, 1.4476, -0.8782, -3.2744]), probability=DenseVector([0.7606, 0.2163, 0.0211, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 8.0, 1121.0, 0.0]), rawPrediction=DenseVector([2.7065, 1.4462, -0.8781, -3.2746]), probability=DenseVector([0.7611, 0.2158, 0.0211, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 9.0, 1121.0, 0.0]), rawPrediction=DenseVector([2.7072, 1.4456, -0.8781, -3.2747]), probability=DenseVector([0.7614, 0.2156, 0.0211, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 10.0, 1121.0, 0.0]), rawPrediction=DenseVector([2.7079, 1.4449, -0.8781, -3.2747]), probability=DenseVector([0.7616, 0.2154, 0.0211, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 14.0, 1121.0, 0.0]), rawPrediction=DenseVector([2.7108, 1.4422, -0.878, -3.275]), probability=DenseVector([0.7626, 0.2145, 0.0211, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 17.0, 1122.0, 0.0]), rawPrediction=DenseVector([2.7133, 1.4403, -0.8782, -3.2754]), probability=DenseVector([0.7633, 0.2137, 0.021, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 18.0, 1122.0, 0.0]), rawPrediction=DenseVector([2.714, 1.4397, -0.8782, -3.2755]), probability=DenseVector([0.7636, 0.2135, 0.021, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 19.0, 1122.0, 0.0]), rawPrediction=DenseVector([2.7147, 1.439, -0.8782, -3.2755]), probability=DenseVector([0.7638, 0.2133, 0.021, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 20.0, 1122.0, 0.0]), rawPrediction=DenseVector([2.7154, 1.4383, -0.8781, -3.2756]), probability=DenseVector([0.764, 0.213, 0.021, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 21.0, 1122.0, 0.0]), rawPrediction=DenseVector([2.7161, 1.4377, -0.8781, -3.2757]), probability=DenseVector([0.7643, 0.2128, 0.021, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 24.0, 1122.0, 0.0]), rawPrediction=DenseVector([2.7183, 1.4357, -0.8781, -3.2759]), probability=DenseVector([0.765, 0.2121, 0.021, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 26.0, 1122.0, 0.0]), rawPrediction=DenseVector([2.7197, 1.4343, -0.878, -3.276]), probability=DenseVector([0.7655, 0.2117, 0.021, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 32.0, 1122.0, 0.0]), rawPrediction=DenseVector([2.724, 1.4303, -0.8779, -3.2765]), probability=DenseVector([0.7669, 0.2103, 0.0209, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 33.0, 1122.0, 0.0]), rawPrediction=DenseVector([2.7248, 1.4297, -0.8779, -3.2765]), probability=DenseVector([0.7671, 0.2101, 0.0209, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 34.0, 1122.0, 0.0]), rawPrediction=DenseVector([2.7255, 1.429, -0.8779, -3.2766]), probability=DenseVector([0.7673, 0.2099, 0.0209, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 40.0, 1123.0, 0.0]), rawPrediction=DenseVector([2.7301, 1.4251, -0.878, -3.2772]), probability=DenseVector([0.7688, 0.2085, 0.0208, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 43.0, 1123.0, 0.0]), rawPrediction=DenseVector([2.7323, 1.4231, -0.878, -3.2774]), probability=DenseVector([0.7695, 0.2078, 0.0208, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 44.0, 1123.0, 0.0]), rawPrediction=DenseVector([2.733, 1.4224, -0.8779, -3.2775]), probability=DenseVector([0.7697, 0.2076, 0.0208, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 45.0, 1123.0, 0.0]), rawPrediction=DenseVector([2.7337, 1.4218, -0.8779, -3.2776]), probability=DenseVector([0.77, 0.2073, 0.0208, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 46.0, 1123.0, 0.0]), rawPrediction=DenseVector([2.7344, 1.4211, -0.8779, -3.2776]), probability=DenseVector([0.7702, 0.2071, 0.0208, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 51.0, 1123.0, 0.0]), rawPrediction=DenseVector([2.738, 1.4177, -0.8778, -3.278]), probability=DenseVector([0.7714, 0.206, 0.0207, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 52.0, 1123.0, 0.0]), rawPrediction=DenseVector([2.7387, 1.4171, -0.8778, -3.2781]), probability=DenseVector([0.7716, 0.2058, 0.0207, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 53.0, 1123.0, 0.0]), rawPrediction=DenseVector([2.7395, 1.4164, -0.8777, -3.2781]), probability=DenseVector([0.7718, 0.2056, 0.0207, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 54.0, 1123.0, 0.0]), rawPrediction=DenseVector([2.7402, 1.4157, -0.8777, -3.2782]), probability=DenseVector([0.7721, 0.2053, 0.0207, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 56.0, 1123.0, 0.0]), rawPrediction=DenseVector([2.7416, 1.4144, -0.8777, -3.2783]), probability=DenseVector([0.7725, 0.2049, 0.0207, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 57.0, 1123.0, 0.0]), rawPrediction=DenseVector([2.7423, 1.4137, -0.8777, -3.2784]), probability=DenseVector([0.7728, 0.2047, 0.0207, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 58.0, 1123.0, 0.0]), rawPrediction=DenseVector([2.7431, 1.4131, -0.8776, -3.2785]), probability=DenseVector([0.773, 0.2044, 0.0207, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 59.0, 1124.0, 0.0]), rawPrediction=DenseVector([2.7441, 1.4125, -0.8779, -3.2787]), probability=DenseVector([0.7733, 0.2042, 0.0207, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 61.0, 1124.0, 0.0]), rawPrediction=DenseVector([2.7455, 1.4112, -0.8779, -3.2789]), probability=DenseVector([0.7737, 0.2037, 0.0207, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 66.0, 1124.0, 0.0]), rawPrediction=DenseVector([2.7491, 1.4079, -0.8778, -3.2792]), probability=DenseVector([0.7749, 0.2026, 0.0206, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 67.0, 1124.0, 0.0]), rawPrediction=DenseVector([2.7498, 1.4072, -0.8777, -3.2793]), probability=DenseVector([0.7751, 0.2024, 0.0206, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 69.0, 1124.0, 0.0]), rawPrediction=DenseVector([2.7513, 1.4058, -0.8777, -3.2794]), probability=DenseVector([0.7756, 0.202, 0.0206, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 70.0, 1124.0, 0.0]), rawPrediction=DenseVector([2.752, 1.4052, -0.8777, -3.2795]), probability=DenseVector([0.7758, 0.2018, 0.0206, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 72.0, 1124.0, 0.0]), rawPrediction=DenseVector([2.7534, 1.4038, -0.8776, -3.2796]), probability=DenseVector([0.7763, 0.2013, 0.0206, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 75.0, 1124.0, 0.0]), rawPrediction=DenseVector([2.7556, 1.4018, -0.8776, -3.2799]), probability=DenseVector([0.7769, 0.2007, 0.0205, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 76.0, 1124.0, 0.0]), rawPrediction=DenseVector([2.7563, 1.4012, -0.8776, -3.2799]), probability=DenseVector([0.7772, 0.2004, 0.0205, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 82.0, 1125.0, 0.0]), rawPrediction=DenseVector([2.7609, 1.3973, -0.8777, -3.2805]), probability=DenseVector([0.7786, 0.1991, 0.0205, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 83.0, 1125.0, 0.0]), rawPrediction=DenseVector([2.7617, 1.3966, -0.8777, -3.2806]), probability=DenseVector([0.7788, 0.1989, 0.0205, 0.0019]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 96.0, 1125.0, 0.0]), rawPrediction=DenseVector([2.771, 1.3879, -0.8774, -3.2815]), probability=DenseVector([0.7817, 0.1961, 0.0204, 0.0018]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 98.0, 1125.0, 0.0]), rawPrediction=DenseVector([2.7725, 1.3866, -0.8774, -3.2817]), probability=DenseVector([0.7822, 0.1956, 0.0203, 0.0018]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 103.0, 1126.0, 0.0]), rawPrediction=DenseVector([2.7764, 1.3834, -0.8776, -3.2822]), probability=DenseVector([0.7834, 0.1945, 0.0203, 0.0018]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 105.0, 1126.0, 0.0]), rawPrediction=DenseVector([2.7778, 1.382, -0.8775, -3.2823]), probability=DenseVector([0.7838, 0.1941, 0.0203, 0.0018]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 110.0, 1126.0, 0.0]), rawPrediction=DenseVector([2.7814, 1.3787, -0.8774, -3.2827]), probability=DenseVector([0.7849, 0.193, 0.0202, 0.0018]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 112.0, 1126.0, 0.0]), rawPrediction=DenseVector([2.7828, 1.3774, -0.8774, -3.2828]), probability=DenseVector([0.7854, 0.1926, 0.0202, 0.0018]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 113.0, 1126.0, 0.0]), rawPrediction=DenseVector([2.7836, 1.3767, -0.8774, -3.2829]), probability=DenseVector([0.7856, 0.1924, 0.0202, 0.0018]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 115.0, 1126.0, 0.0]), rawPrediction=DenseVector([2.785, 1.3754, -0.8773, -3.283]), probability=DenseVector([0.786, 0.192, 0.0202, 0.0018]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 117.0, 1126.0, 0.0]), rawPrediction=DenseVector([2.7864, 1.374, -0.8773, -3.2832]), probability=DenseVector([0.7865, 0.1916, 0.0202, 0.0018]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 121.0, 1127.0, 0.0]), rawPrediction=DenseVector([2.7896, 1.3715, -0.8775, -3.2836]), probability=DenseVector([0.7874, 0.1907, 0.0201, 0.0018]), prediction=0.0),\n Row(label=1, features=DenseVector([477.0, 124.0, 1127.0, 0.0]), rawPrediction=DenseVector([2.7918, 1.3695, -0.8774, -3.2838]), probability=DenseVector([0.7881, 0.19, 0.0201, 0.0018]), prediction=0.0)]"
                    },
                    "execution_count": 43,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "lr_results.filter(lr_results[\"label\"]==1).head(50)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "lrmodel = lr.fit(train)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "lrmodel.summary.accuracy"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "lrmodel.summary.precisionByLabel"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Optimised"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# pipeline, just running it on classifier no transformations\npipeline = Pipeline(stages=[lr])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# set up param grid to iterate over\nparamGrid = ParamGridBuilder() \\\n.addGrid(lr.regParam, [0.0, 0.1]) \\\n.build()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# set up crossvalidator to tune parameters and optimize\ncrossval = CrossValidator(estimator=pipeline,\n                         estimatorParamMaps=paramGrid,\n                         evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n                         numFolds=2)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "cvModel = crossval.fit(train)  # train model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "results = cvModel.transform(test)  # apply model on test data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "cvModel.avgMetrics  # look at model scoring metrics"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "results.count()  # how many events in total labels"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(results.filter(results.label == results.prediction).count())  # check how many were predicted correctly"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "results.filter(results.label == results.prediction).count()/results.count()   # hwow many correct"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "results.filter(results[\"prediction\"]==1).head(5)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "results.filter(results[\"label\"]==1).head(50)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}